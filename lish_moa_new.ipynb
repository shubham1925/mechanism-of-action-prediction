{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lish_moa_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeY8zdHrs7LP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c36082-3c46-4d3b-fff7-1a96f720e4ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1MOLBLWX4-d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend\n",
        "import tensorflow.keras.layers\n",
        "import tensorflow.keras.models\n",
        "import time\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kESwrSmI2RJK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "d8ec1f4d-bf0b-4e2c-f111-389ceebbc9bc"
      },
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/lish-moa/train_features.csv')\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/lish-moa/test_features.csv')\n",
        "targetns = pd.read_csv('/content/drive/MyDrive/lish-moa/train_targets_nonscored.csv')\n",
        "targetscored = pd.read_csv('/content/drive/MyDrive/lish-moa/train_targets_scored.csv')\n",
        "data_train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4805</td>\n",
              "      <td>0.4965</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4083</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.7099</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5477</td>\n",
              "      <td>-0.7576</td>\n",
              "      <td>-0.0444</td>\n",
              "      <td>0.1894</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_0015fd391</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.5138</td>\n",
              "      <td>-0.2491</td>\n",
              "      <td>-0.2656</td>\n",
              "      <td>0.5288</td>\n",
              "      <td>4.0620</td>\n",
              "      <td>-0.8095</td>\n",
              "      <td>-1.9590</td>\n",
              "      <td>0.1792</td>\n",
              "      <td>-0.1321</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.8269</td>\n",
              "      <td>-0.3584</td>\n",
              "      <td>-0.8511</td>\n",
              "      <td>-0.5844</td>\n",
              "      <td>-2.5690</td>\n",
              "      <td>0.8183</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.8554</td>\n",
              "      <td>0.1160</td>\n",
              "      <td>-2.3520</td>\n",
              "      <td>2.1200</td>\n",
              "      <td>-1.1580</td>\n",
              "      <td>-0.7191</td>\n",
              "      <td>-0.8004</td>\n",
              "      <td>-1.4670</td>\n",
              "      <td>-0.0107</td>\n",
              "      <td>-0.8995</td>\n",
              "      <td>0.2406</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-1.0890</td>\n",
              "      <td>-0.7575</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>-2.7370</td>\n",
              "      <td>0.8745</td>\n",
              "      <td>0.5787</td>\n",
              "      <td>-1.6740</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.1220</td>\n",
              "      <td>-0.3752</td>\n",
              "      <td>-2.3820</td>\n",
              "      <td>-3.7350</td>\n",
              "      <td>-2.9740</td>\n",
              "      <td>-1.4930</td>\n",
              "      <td>-1.6600</td>\n",
              "      <td>-3.1660</td>\n",
              "      <td>0.2816</td>\n",
              "      <td>-0.2990</td>\n",
              "      <td>-1.1870</td>\n",
              "      <td>-0.5044</td>\n",
              "      <td>-1.7750</td>\n",
              "      <td>-1.6120</td>\n",
              "      <td>-0.9215</td>\n",
              "      <td>-1.0810</td>\n",
              "      <td>-3.0520</td>\n",
              "      <td>-3.4470</td>\n",
              "      <td>-2.7740</td>\n",
              "      <td>-1.8460</td>\n",
              "      <td>-0.5568</td>\n",
              "      <td>-3.3960</td>\n",
              "      <td>-2.9510</td>\n",
              "      <td>-1.1550</td>\n",
              "      <td>-3.2620</td>\n",
              "      <td>-1.5390</td>\n",
              "      <td>-2.4600</td>\n",
              "      <td>-0.9417</td>\n",
              "      <td>-1.5550</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>-2.0990</td>\n",
              "      <td>-0.6441</td>\n",
              "      <td>-5.6300</td>\n",
              "      <td>-1.3780</td>\n",
              "      <td>-0.8632</td>\n",
              "      <td>-1.2880</td>\n",
              "      <td>-1.6210</td>\n",
              "      <td>-0.8784</td>\n",
              "      <td>-0.3876</td>\n",
              "      <td>-0.8154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_001626bd3</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D2</td>\n",
              "      <td>-0.3254</td>\n",
              "      <td>-0.4009</td>\n",
              "      <td>0.9700</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>1.4180</td>\n",
              "      <td>-0.8244</td>\n",
              "      <td>-0.2800</td>\n",
              "      <td>-0.1498</td>\n",
              "      <td>-0.8789</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>-0.2219</td>\n",
              "      <td>-0.5121</td>\n",
              "      <td>-0.9577</td>\n",
              "      <td>1.1750</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1244</td>\n",
              "      <td>-1.7090</td>\n",
              "      <td>-0.3543</td>\n",
              "      <td>-0.5160</td>\n",
              "      <td>-0.3330</td>\n",
              "      <td>-0.2685</td>\n",
              "      <td>0.7649</td>\n",
              "      <td>0.2057</td>\n",
              "      <td>1.3720</td>\n",
              "      <td>0.6835</td>\n",
              "      <td>0.8056</td>\n",
              "      <td>-0.3754</td>\n",
              "      <td>-1.2090</td>\n",
              "      <td>0.2965</td>\n",
              "      <td>-0.0712</td>\n",
              "      <td>0.6389</td>\n",
              "      <td>0.6674</td>\n",
              "      <td>-0.0783</td>\n",
              "      <td>1.1740</td>\n",
              "      <td>-0.7110</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2274</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1535</td>\n",
              "      <td>-0.4640</td>\n",
              "      <td>-0.5943</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.1500</td>\n",
              "      <td>0.5178</td>\n",
              "      <td>0.5159</td>\n",
              "      <td>0.6091</td>\n",
              "      <td>0.1813</td>\n",
              "      <td>-0.4249</td>\n",
              "      <td>0.7832</td>\n",
              "      <td>0.6529</td>\n",
              "      <td>0.5648</td>\n",
              "      <td>0.4817</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.6376</td>\n",
              "      <td>-0.3966</td>\n",
              "      <td>-1.4950</td>\n",
              "      <td>-0.9625</td>\n",
              "      <td>-0.0541</td>\n",
              "      <td>0.6273</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.0698</td>\n",
              "      <td>0.8134</td>\n",
              "      <td>0.1924</td>\n",
              "      <td>0.6054</td>\n",
              "      <td>-0.1824</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.6670</td>\n",
              "      <td>1.0690</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.3031</td>\n",
              "      <td>0.1094</td>\n",
              "      <td>0.2885</td>\n",
              "      <td>-0.3786</td>\n",
              "      <td>0.7125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23809</th>\n",
              "      <td>id_fffb1ceed</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D2</td>\n",
              "      <td>0.1394</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.1112</td>\n",
              "      <td>-0.5080</td>\n",
              "      <td>-0.4713</td>\n",
              "      <td>0.7201</td>\n",
              "      <td>0.5773</td>\n",
              "      <td>0.3055</td>\n",
              "      <td>-0.4726</td>\n",
              "      <td>0.1269</td>\n",
              "      <td>0.2531</td>\n",
              "      <td>0.1730</td>\n",
              "      <td>-0.4532</td>\n",
              "      <td>-1.0790</td>\n",
              "      <td>0.2474</td>\n",
              "      <td>-0.4550</td>\n",
              "      <td>0.3588</td>\n",
              "      <td>0.1600</td>\n",
              "      <td>-0.7362</td>\n",
              "      <td>-0.1103</td>\n",
              "      <td>0.8550</td>\n",
              "      <td>-0.4139</td>\n",
              "      <td>0.5541</td>\n",
              "      <td>0.2310</td>\n",
              "      <td>-0.5573</td>\n",
              "      <td>-0.4397</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.2424</td>\n",
              "      <td>-0.6686</td>\n",
              "      <td>0.2326</td>\n",
              "      <td>0.6456</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>-0.5141</td>\n",
              "      <td>-0.6320</td>\n",
              "      <td>0.7166</td>\n",
              "      <td>-0.1736</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0807</td>\n",
              "      <td>0.4024</td>\n",
              "      <td>-0.0895</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9641</td>\n",
              "      <td>-0.1846</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>0.3154</td>\n",
              "      <td>-0.2071</td>\n",
              "      <td>-0.6158</td>\n",
              "      <td>-0.2977</td>\n",
              "      <td>0.0992</td>\n",
              "      <td>0.6838</td>\n",
              "      <td>0.5259</td>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3119</td>\n",
              "      <td>-0.7697</td>\n",
              "      <td>0.2203</td>\n",
              "      <td>-1.0710</td>\n",
              "      <td>0.5979</td>\n",
              "      <td>0.0848</td>\n",
              "      <td>-0.2555</td>\n",
              "      <td>0.6293</td>\n",
              "      <td>1.1660</td>\n",
              "      <td>0.3329</td>\n",
              "      <td>0.2754</td>\n",
              "      <td>0.4108</td>\n",
              "      <td>-0.1252</td>\n",
              "      <td>-0.2340</td>\n",
              "      <td>0.2267</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>-0.8121</td>\n",
              "      <td>0.3434</td>\n",
              "      <td>0.5372</td>\n",
              "      <td>-0.3246</td>\n",
              "      <td>0.0631</td>\n",
              "      <td>0.9171</td>\n",
              "      <td>0.5258</td>\n",
              "      <td>0.4680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23810</th>\n",
              "      <td>id_fffb70c0c</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D2</td>\n",
              "      <td>-1.3260</td>\n",
              "      <td>0.3478</td>\n",
              "      <td>-0.3743</td>\n",
              "      <td>0.9905</td>\n",
              "      <td>-0.7178</td>\n",
              "      <td>0.6621</td>\n",
              "      <td>-0.2252</td>\n",
              "      <td>-0.5565</td>\n",
              "      <td>0.5112</td>\n",
              "      <td>0.6727</td>\n",
              "      <td>-0.1851</td>\n",
              "      <td>2.8650</td>\n",
              "      <td>-0.2140</td>\n",
              "      <td>-0.6153</td>\n",
              "      <td>0.8362</td>\n",
              "      <td>0.5584</td>\n",
              "      <td>-0.2589</td>\n",
              "      <td>0.1292</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0949</td>\n",
              "      <td>-0.2182</td>\n",
              "      <td>-0.9235</td>\n",
              "      <td>0.0749</td>\n",
              "      <td>-1.5910</td>\n",
              "      <td>-0.8359</td>\n",
              "      <td>-0.9217</td>\n",
              "      <td>0.3013</td>\n",
              "      <td>0.1716</td>\n",
              "      <td>0.0880</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.1835</td>\n",
              "      <td>0.5436</td>\n",
              "      <td>-0.0533</td>\n",
              "      <td>-0.0491</td>\n",
              "      <td>0.9543</td>\n",
              "      <td>0.4626</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1410</td>\n",
              "      <td>1.2640</td>\n",
              "      <td>-0.8663</td>\n",
              "      <td>0.8129</td>\n",
              "      <td>-0.1514</td>\n",
              "      <td>-0.4652</td>\n",
              "      <td>-0.7390</td>\n",
              "      <td>-1.3270</td>\n",
              "      <td>0.9925</td>\n",
              "      <td>1.0570</td>\n",
              "      <td>-0.3355</td>\n",
              "      <td>-0.2555</td>\n",
              "      <td>0.8219</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>0.2408</td>\n",
              "      <td>-0.7781</td>\n",
              "      <td>-0.0929</td>\n",
              "      <td>-0.0329</td>\n",
              "      <td>0.0781</td>\n",
              "      <td>-1.4440</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>0.3188</td>\n",
              "      <td>-1.1080</td>\n",
              "      <td>0.4895</td>\n",
              "      <td>-0.2144</td>\n",
              "      <td>1.0960</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.4444</td>\n",
              "      <td>-1.1130</td>\n",
              "      <td>0.4286</td>\n",
              "      <td>0.4426</td>\n",
              "      <td>0.0423</td>\n",
              "      <td>-0.3195</td>\n",
              "      <td>-0.8086</td>\n",
              "      <td>-0.9798</td>\n",
              "      <td>-0.2084</td>\n",
              "      <td>-0.1224</td>\n",
              "      <td>-0.2715</td>\n",
              "      <td>0.3689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23811</th>\n",
              "      <td>id_fffc1c3f4</td>\n",
              "      <td>ctl_vehicle</td>\n",
              "      <td>48</td>\n",
              "      <td>D2</td>\n",
              "      <td>0.3942</td>\n",
              "      <td>0.3756</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>-0.7389</td>\n",
              "      <td>0.5505</td>\n",
              "      <td>-0.0159</td>\n",
              "      <td>-0.2541</td>\n",
              "      <td>0.1745</td>\n",
              "      <td>-0.0340</td>\n",
              "      <td>0.4865</td>\n",
              "      <td>-0.1854</td>\n",
              "      <td>0.0716</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>-0.0434</td>\n",
              "      <td>0.1542</td>\n",
              "      <td>-0.2192</td>\n",
              "      <td>-0.0302</td>\n",
              "      <td>-0.4218</td>\n",
              "      <td>0.4057</td>\n",
              "      <td>-0.5372</td>\n",
              "      <td>0.1521</td>\n",
              "      <td>-0.2651</td>\n",
              "      <td>0.2310</td>\n",
              "      <td>-0.8101</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.6905</td>\n",
              "      <td>-0.3720</td>\n",
              "      <td>-1.4110</td>\n",
              "      <td>0.4516</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.1949</td>\n",
              "      <td>-1.3280</td>\n",
              "      <td>-0.4276</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>-0.3086</td>\n",
              "      <td>-0.2355</td>\n",
              "      <td>...</td>\n",
              "      <td>0.6845</td>\n",
              "      <td>0.7127</td>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.4718</td>\n",
              "      <td>-0.2020</td>\n",
              "      <td>0.2783</td>\n",
              "      <td>0.4934</td>\n",
              "      <td>0.4144</td>\n",
              "      <td>0.5449</td>\n",
              "      <td>1.4690</td>\n",
              "      <td>-0.6142</td>\n",
              "      <td>0.6068</td>\n",
              "      <td>0.3434</td>\n",
              "      <td>0.9880</td>\n",
              "      <td>-0.0468</td>\n",
              "      <td>-0.1882</td>\n",
              "      <td>-0.0087</td>\n",
              "      <td>-0.0356</td>\n",
              "      <td>0.5718</td>\n",
              "      <td>0.4971</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.6992</td>\n",
              "      <td>0.0708</td>\n",
              "      <td>0.6169</td>\n",
              "      <td>0.2248</td>\n",
              "      <td>0.5994</td>\n",
              "      <td>0.2689</td>\n",
              "      <td>0.0305</td>\n",
              "      <td>1.2320</td>\n",
              "      <td>0.5409</td>\n",
              "      <td>0.3755</td>\n",
              "      <td>0.7343</td>\n",
              "      <td>0.2807</td>\n",
              "      <td>0.4116</td>\n",
              "      <td>0.6422</td>\n",
              "      <td>0.2256</td>\n",
              "      <td>0.7592</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.3808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23812</th>\n",
              "      <td>id_fffcb9e7c</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6660</td>\n",
              "      <td>0.2324</td>\n",
              "      <td>0.4392</td>\n",
              "      <td>0.2044</td>\n",
              "      <td>0.8531</td>\n",
              "      <td>-0.0343</td>\n",
              "      <td>0.0323</td>\n",
              "      <td>0.0463</td>\n",
              "      <td>0.4299</td>\n",
              "      <td>-0.7985</td>\n",
              "      <td>0.5742</td>\n",
              "      <td>0.1421</td>\n",
              "      <td>2.2700</td>\n",
              "      <td>0.2046</td>\n",
              "      <td>0.5363</td>\n",
              "      <td>-1.7330</td>\n",
              "      <td>0.1450</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.2024</td>\n",
              "      <td>0.9865</td>\n",
              "      <td>-0.7805</td>\n",
              "      <td>0.9608</td>\n",
              "      <td>0.3440</td>\n",
              "      <td>2.7650</td>\n",
              "      <td>0.4925</td>\n",
              "      <td>0.6698</td>\n",
              "      <td>0.2374</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.8771</td>\n",
              "      <td>-2.6560</td>\n",
              "      <td>-0.2000</td>\n",
              "      <td>-0.2043</td>\n",
              "      <td>0.6797</td>\n",
              "      <td>-0.0248</td>\n",
              "      <td>-0.0927</td>\n",
              "      <td>1.8480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.3360</td>\n",
              "      <td>-0.6136</td>\n",
              "      <td>0.5011</td>\n",
              "      <td>0.9261</td>\n",
              "      <td>0.4419</td>\n",
              "      <td>0.0295</td>\n",
              "      <td>0.4220</td>\n",
              "      <td>0.4677</td>\n",
              "      <td>-0.1184</td>\n",
              "      <td>0.4524</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.1356</td>\n",
              "      <td>-0.5801</td>\n",
              "      <td>0.0411</td>\n",
              "      <td>1.0240</td>\n",
              "      <td>1.0340</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.4194</td>\n",
              "      <td>0.7403</td>\n",
              "      <td>-0.6793</td>\n",
              "      <td>-0.1423</td>\n",
              "      <td>0.7307</td>\n",
              "      <td>0.7946</td>\n",
              "      <td>-0.0650</td>\n",
              "      <td>0.9038</td>\n",
              "      <td>0.2324</td>\n",
              "      <td>0.9676</td>\n",
              "      <td>1.0940</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.5187</td>\n",
              "      <td>-0.1105</td>\n",
              "      <td>0.4258</td>\n",
              "      <td>-0.2012</td>\n",
              "      <td>0.1506</td>\n",
              "      <td>1.5230</td>\n",
              "      <td>0.7101</td>\n",
              "      <td>0.1732</td>\n",
              "      <td>0.7015</td>\n",
              "      <td>-0.6290</td>\n",
              "      <td>0.0740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23813</th>\n",
              "      <td>id_ffffdd77b</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.8598</td>\n",
              "      <td>1.0240</td>\n",
              "      <td>-0.1361</td>\n",
              "      <td>0.7952</td>\n",
              "      <td>-0.3611</td>\n",
              "      <td>-3.6750</td>\n",
              "      <td>-1.2420</td>\n",
              "      <td>0.9146</td>\n",
              "      <td>3.0790</td>\n",
              "      <td>1.2460</td>\n",
              "      <td>1.9460</td>\n",
              "      <td>1.4370</td>\n",
              "      <td>2.9780</td>\n",
              "      <td>2.2370</td>\n",
              "      <td>-0.6818</td>\n",
              "      <td>0.6870</td>\n",
              "      <td>-1.1060</td>\n",
              "      <td>0.0182</td>\n",
              "      <td>-0.9247</td>\n",
              "      <td>-0.0738</td>\n",
              "      <td>-0.1919</td>\n",
              "      <td>-0.7722</td>\n",
              "      <td>-1.4050</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-1.1170</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-1.1720</td>\n",
              "      <td>-0.2885</td>\n",
              "      <td>0.1599</td>\n",
              "      <td>-0.4250</td>\n",
              "      <td>0.3591</td>\n",
              "      <td>-0.1420</td>\n",
              "      <td>-0.9530</td>\n",
              "      <td>-0.2005</td>\n",
              "      <td>-1.8340</td>\n",
              "      <td>-0.5768</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.9170</td>\n",
              "      <td>-1.8640</td>\n",
              "      <td>-2.5090</td>\n",
              "      <td>-4.7130</td>\n",
              "      <td>-1.7250</td>\n",
              "      <td>-3.8650</td>\n",
              "      <td>-3.0800</td>\n",
              "      <td>-4.1530</td>\n",
              "      <td>-1.2030</td>\n",
              "      <td>-1.1690</td>\n",
              "      <td>-4.1460</td>\n",
              "      <td>-1.2670</td>\n",
              "      <td>-1.1300</td>\n",
              "      <td>-2.4390</td>\n",
              "      <td>0.1591</td>\n",
              "      <td>-2.2490</td>\n",
              "      <td>-2.5860</td>\n",
              "      <td>-1.9520</td>\n",
              "      <td>-2.1810</td>\n",
              "      <td>-4.6690</td>\n",
              "      <td>-3.9450</td>\n",
              "      <td>-2.9560</td>\n",
              "      <td>-2.7930</td>\n",
              "      <td>-2.1560</td>\n",
              "      <td>-2.4100</td>\n",
              "      <td>-1.8190</td>\n",
              "      <td>-3.3480</td>\n",
              "      <td>-0.1414</td>\n",
              "      <td>-2.6430</td>\n",
              "      <td>-2.5810</td>\n",
              "      <td>-3.3890</td>\n",
              "      <td>-1.7450</td>\n",
              "      <td>-6.6300</td>\n",
              "      <td>-4.0950</td>\n",
              "      <td>-7.3860</td>\n",
              "      <td>-1.4160</td>\n",
              "      <td>-3.5770</td>\n",
              "      <td>-0.4775</td>\n",
              "      <td>-2.1500</td>\n",
              "      <td>-4.2520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23814 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id      cp_type  cp_time  ...    c-97    c-98    c-99\n",
              "0      id_000644bb2       trt_cp       24  ...  0.2139  0.3801  0.4176\n",
              "1      id_000779bfc       trt_cp       72  ...  0.1241  0.6077  0.7371\n",
              "2      id_000a6266a       trt_cp       48  ... -0.2187 -1.4080  0.6931\n",
              "3      id_0015fd391       trt_cp       48  ... -0.8784 -0.3876 -0.8154\n",
              "4      id_001626bd3       trt_cp       72  ...  0.2885 -0.3786  0.7125\n",
              "...             ...          ...      ...  ...     ...     ...     ...\n",
              "23809  id_fffb1ceed       trt_cp       24  ...  0.9171  0.5258  0.4680\n",
              "23810  id_fffb70c0c       trt_cp       24  ... -0.1224 -0.2715  0.3689\n",
              "23811  id_fffc1c3f4  ctl_vehicle       48  ...  0.7592  0.6656  0.3808\n",
              "23812  id_fffcb9e7c       trt_cp       24  ...  0.7015 -0.6290  0.0740\n",
              "23813  id_ffffdd77b       trt_cp       72  ... -0.4775 -2.1500 -4.2520\n",
              "\n",
              "[23814 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVDxQfKS3AXe",
        "outputId": "25da8831-52cd-4ce7-92bc-93f5e0f17858"
      },
      "source": [
        "target_cols = targetscored.columns[1:]\n",
        "# N_TARGETS = len(target_cols)\n",
        "print(data_train.shape)\n",
        "print(data_test.shape)\n",
        "print(targetns.shape)\n",
        "print(targetscored.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23814, 876)\n",
            "(3982, 876)\n",
            "(23814, 403)\n",
            "(23814, 207)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Nvks1Z--8o"
      },
      "source": [
        "cells = [col for col in data_train.columns if col.startswith('c-')]\n",
        "genes = [col for col in data_train.columns if col.startswith('g-')]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ZwI4Nla5-N0r",
        "outputId": "5dbb7d3d-b222-461d-ada8-626a66efad2a"
      },
      "source": [
        "#Deciding the number of components for gene features\r\n",
        "data = pd.concat([pd.DataFrame(data_train[genes]), pd.DataFrame(data_test[genes])])\r\n",
        "pca = PCA().fit(data)\r\n",
        "xi = np.arange(1, 773, 1)\r\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\r\n",
        "plt.plot(xi, y, color = 'red')\r\n",
        "plt.axhline(y = 0.95, color = 'blue')\r\n",
        "plt.grid(axis = 'x')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sKWKH-HG-PFn",
        "outputId": "4b502cf2-275a-4108-8337-55103ebf4f84"
      },
      "source": [
        "#Deciding the number of components for cell features\r\n",
        "data = pd.concat([pd.DataFrame(data_train[cells]), pd.DataFrame(data_test[cells])])\r\n",
        "pca = PCA().fit(data)\r\n",
        "xi = np.arange(1, 101, 1)\r\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\r\n",
        "plt.plot(xi, y, color = 'red')\r\n",
        "plt.axhline(y = 0.95, color = 'blue')\r\n",
        "plt.grid(axis = 'x')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2De-DZaFa5-3"
      },
      "source": [
        "# For g- features\n",
        "data = pd.concat([pd.DataFrame(data_train[genes]), pd.DataFrame(data_test[genes])])\n",
        "data_pca = (PCA(n_components = 525, random_state = 100).fit_transform(data[genes]))\n",
        "train_pca = data_pca[:data_train.shape[0]]\n",
        "test_pca = data_pca[-data_test.shape[0]:]\n",
        "\n",
        "train_pca = pd.DataFrame(train_pca, columns = [f'pca_G-{i}' for i in range(525)])\n",
        "test_pca = pd.DataFrame(test_pca, columns = [f'pca_G-{i}' for i in range(525)])\n",
        "\n",
        "data_train = pd.concat((data_train, train_pca), axis = 1)\n",
        "data_test = pd.concat((data_test, test_pca), axis = 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPGwe49ac8CM"
      },
      "source": [
        "# For c- features\n",
        "data = pd.concat([pd.DataFrame(data_train[cells]), pd.DataFrame(data_test[cells])])\n",
        "data_pca = (PCA(n_components = 45, random_state = 100).fit_transform(data[cells]))\n",
        "train_pca = data_pca[:data_train.shape[0]]\n",
        "test_pca = data_pca[-data_test.shape[0]:]\n",
        "\n",
        "train_pca = pd.DataFrame(train_pca, columns = [f'pca_C-{i}' for i in range(45)])\n",
        "test_pca = pd.DataFrame(test_pca, columns = [f'pca_C-{i}' for i in range(45)])\n",
        "data_train = pd.concat((data_train, train_pca), axis = 1)\n",
        "data_test = pd.concat((data_test, test_pca), axis = 1)\n",
        "# data_train"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YlRDw0xdiof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015f8d76-5c98-45e6-c9d9-b53ed5d16fd9"
      },
      "source": [
        "train_copy = data_train\n",
        "var_thresh = VarianceThreshold(0.8)\n",
        "data = data_train.append(data_test)\n",
        "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
        "data_transformed"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.062     , -0.2479    , -0.6208    , ..., -0.88945237,\n",
              "         0.52050405, -0.41983391],\n",
              "       [ 0.0743    ,  0.2991    ,  0.0604    , ...,  0.22471968,\n",
              "         0.94895294, -0.05855933],\n",
              "       [ 0.628     ,  1.554     , -0.0764    , ..., -0.13469001,\n",
              "        -0.2249024 ,  0.16776555],\n",
              "       ...,\n",
              "       [-0.3985    ,  0.2677    , -0.6813    , ..., -0.12958965,\n",
              "        -0.27791464,  0.999736  ],\n",
              "       [-1.096     , -0.3977    ,  1.016     , ..., -0.10545347,\n",
              "         0.01487324, -0.08750892],\n",
              "       [-0.5174    ,  0.3286    , -0.0428    , ..., -0.47032949,\n",
              "         0.35373542, -0.21689743]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1kT7TyeRnV"
      },
      "source": [
        "train_df_trans = data_transformed[ : data_train.shape[0]]\n",
        "test_df_trans = data_transformed[-data_test.shape[0] : ]\n",
        "\n",
        "data_train = pd.DataFrame(data_train[['sig_id', 'cp_type', 'cp_time', 'cp_dose']].\n",
        "                        values.reshape(-1, 4), columns = ['sig_id', 'cp_type', \n",
        "                                                          'cp_time', 'cp_dose'])\n",
        "# data_train.head\n",
        "data_train = pd.concat([data_train, pd.DataFrame(train_df_trans)], axis = 1)\n",
        "\n",
        "\n",
        "data_test = pd.DataFrame(data_test[['sig_id', 'cp_type', 'cp_time', 'cp_dose']].\n",
        "                       values.reshape(-1, 4), columns = ['sig_id', 'cp_type', \n",
        "                                                         'cp_time', 'cp_dose'])\n",
        "# data_train.head\n",
        "data_test = pd.concat([data_test, pd.DataFrame(test_df_trans)], axis = 1)\n",
        "# data_train.head"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdNyeb33pzF-"
      },
      "source": [
        "def preprocess_df(data):\n",
        "  # data['cp_type'] = (data['cp_type'] == 'trt_cp').astype(int)\n",
        "  # data['cp_dose'] = (data['cp_dose'] == 'D2').astype(int)\n",
        "  data.drop(['cp_type'], axis = 1, inplace = True)\n",
        "  data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1':0, 'D2':1})\n",
        "  data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24:0, 48:1, 72:2})\n",
        "  return data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIz-efmeqG97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2519ce38-0858-4f9d-85aa-8fb7a953ceeb"
      },
      "source": [
        "x_train = preprocess_df(data_train.drop(columns = \"sig_id\"))\n",
        "x_test = preprocess_df(data_test.drop(columns = \"sig_id\"))\n",
        "y_train = targetscored.drop(columns = \"sig_id\")\n",
        "y_test = targetns.drop(columns = \"sig_id\")\n",
        "print(y_test.shape)\n",
        "N_FEATURES = x_train.shape[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23814, 402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xytYuIpmqnTZ"
      },
      "source": [
        "x_train = x_train.astype({'cp_time':int})\n",
        "x_test = x_test.astype({'cp_time':int})\n",
        "x_train_validation = x_train.copy()\n",
        "# x_train"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pa34J9WZs9y"
      },
      "source": [
        "def create_model(col_num):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Input(col_num))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(6144, \n",
        "                                                                 activation = \"relu\")))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(6144, \n",
        "                                                                 activation = \"relu\")))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, \n",
        "                                                                 activation = \"sigmoid\")))\n",
        "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005), \n",
        "                loss = 'binary_crossentropy', metrics = [\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBfRn_xRDDC"
      },
      "source": [
        "def build_train(repeat_number = 0, folds = 5):\n",
        "  models = []\n",
        "  predicted_values = y_train.copy()\n",
        "  kfold = KFold(n_splits = folds, shuffle = True)\n",
        "  for fold, (ti, vi) in enumerate(kfold.split(x_train)):\n",
        "    print('Training fold: ', fold)\n",
        "    model_save = f'model:{repeat_number}_{fold}.hdf5'\n",
        "    callback_1 = tf.keras.callbacks.ModelCheckpoint(model_save, \n",
        "                                                    monitor = 'val_loss', \n",
        "                                                    verbose = 2,\n",
        "                                                    save_best_only = True, \n",
        "                                                    save_weights_only = True, \n",
        "                                                    mode = 'min')\n",
        "    \n",
        "    callback_2 = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n",
        "                                                          factor = 0.4, \n",
        "                                                          patience = 2, \n",
        "                                                          verbose = 1, \n",
        "                                                          min_delta = 0.0001, \n",
        "                                                          mode = 'auto')   \n",
        "    \n",
        "    # callback_3 = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', \n",
        "    #                                               verbose = 2, mode = 'max', \n",
        "    #                                               min_delta = 0.01)\n",
        "\n",
        "    model = create_model(N_FEATURES)\n",
        "    training_history = model.fit(x_train.values[ti], \n",
        "                                 y_train.values[ti], \n",
        "              validation_data = (x_train.values[vi], y_train.values[vi]),\n",
        "              callbacks = [callback_1, callback_2], epochs = 20, \n",
        "              batch_size = 128, verbose = 2)\n",
        "    model.load_weights(model_save)\n",
        "    predicted_values.loc[vi, :] = model.predict(x_train.values[vi])\n",
        "    models.append(model)\n",
        "    print('train:')\n",
        "    print(list(zip(model.metrics_names, model.evaluate(x_train.values[ti], \n",
        "                                                       y_train.values[ti], \n",
        "                                                       verbose = 0, \n",
        "                                                       batch_size = 32))))\n",
        "    print('val:')\n",
        "    print(list(zip(model.metrics_names, model.evaluate(x_train.values[vi], \n",
        "                                                       y_train.values[vi], \n",
        "                                                       verbose = 0, \n",
        "                                                       batch_size = 32))))\n",
        "    return models, predicted_values, training_history\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh4Y1LfDVHnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ed991c-a748-4519-bcbc-e4b585e6e56c"
      },
      "source": [
        "model = create_model(col_num = N_FEATURES)\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "weight_normalization (Weight (None, 6144)              12601345  \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6144)              24576     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6144)              0         \n",
            "_________________________________________________________________\n",
            "weight_normalization_1 (Weig (None, 6144)              75515905  \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 6144)              24576     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6144)              0         \n",
            "_________________________________________________________________\n",
            "weight_normalization_2 (Weig (None, 206)               2531947   \n",
            "=================================================================\n",
            "Total params: 90,702,445\n",
            "Trainable params: 45,357,468\n",
            "Non-trainable params: 45,344,977\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYphaA8kSYbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf26d14d-e485-4aaa-95d0-47ea11f8ccb5"
      },
      "source": [
        "models = []\n",
        "all_predicted = []\n",
        "np.random.seed(11)\n",
        "os.environ['PYTHONHASHSEED'] = str(11)\n",
        "tf.random.set_seed(11)\n",
        "# SEED_ARRAY = [0, 1, 2, 3, 4]\n",
        "for i in range(5):\n",
        "  m_temp, pred_new, history = build_train(repeat_number = i, folds = 5)  \n",
        "  all_predicted.append(pred_new)\n",
        "  models = models + m_temp"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold:  0\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0321s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10237, saving model to model:0_0.hdf5\n",
            "149/149 - 8s - loss: 0.4571 - accuracy: 0.0454 - val_loss: 0.1024 - val_accuracy: 0.0537\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10237 to 0.02521, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0507 - accuracy: 0.0729 - val_loss: 0.0252 - val_accuracy: 0.1006\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02521 to 0.02104, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0243 - accuracy: 0.0889 - val_loss: 0.0210 - val_accuracy: 0.0771\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02104 to 0.01834, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0203 - accuracy: 0.0956 - val_loss: 0.0183 - val_accuracy: 0.0802\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01834 to 0.01768, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0187 - accuracy: 0.1070 - val_loss: 0.0177 - val_accuracy: 0.1199\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01768 to 0.01707, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0179 - accuracy: 0.1131 - val_loss: 0.0171 - val_accuracy: 0.0913\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01707 to 0.01670, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0173 - accuracy: 0.1183 - val_loss: 0.0167 - val_accuracy: 0.0995\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01670 to 0.01657, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0168 - accuracy: 0.1236 - val_loss: 0.0166 - val_accuracy: 0.1369\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01657 to 0.01618, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0164 - accuracy: 0.1298 - val_loss: 0.0162 - val_accuracy: 0.1346\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01618\n",
            "149/149 - 6s - loss: 0.0161 - accuracy: 0.1383 - val_loss: 0.0162 - val_accuracy: 0.1474\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01618 to 0.01589, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0157 - accuracy: 0.1438 - val_loss: 0.0159 - val_accuracy: 0.1188\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01589 to 0.01585, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0154 - accuracy: 0.1478 - val_loss: 0.0158 - val_accuracy: 0.1214\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01585 to 0.01578, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0151 - accuracy: 0.1511 - val_loss: 0.0158 - val_accuracy: 0.1235\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01578 to 0.01571, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0148 - accuracy: 0.1586 - val_loss: 0.0157 - val_accuracy: 0.1262\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01571 to 0.01555, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0144 - accuracy: 0.1652 - val_loss: 0.0156 - val_accuracy: 0.1258\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01555\n",
            "149/149 - 6s - loss: 0.0143 - accuracy: 0.1721 - val_loss: 0.0158 - val_accuracy: 0.1358\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01555\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "149/149 - 6s - loss: 0.0141 - accuracy: 0.1747 - val_loss: 0.0157 - val_accuracy: 0.1302\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01555 to 0.01539, saving model to model:0_0.hdf5\n",
            "149/149 - 7s - loss: 0.0133 - accuracy: 0.1891 - val_loss: 0.0154 - val_accuracy: 0.1300\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01539\n",
            "149/149 - 6s - loss: 0.0130 - accuracy: 0.1981 - val_loss: 0.0155 - val_accuracy: 0.1321\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01539\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
            "149/149 - 6s - loss: 0.0127 - accuracy: 0.2033 - val_loss: 0.0154 - val_accuracy: 0.1335\n",
            "train:\n",
            "[('loss', 0.011041379533708096), ('accuracy', 0.2638706564903259)]\n",
            "val:\n",
            "[('loss', 0.015389320440590382), ('accuracy', 0.1299601048231125)]\n",
            "Training fold:  0\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0188s vs `on_train_batch_end` time: 0.0319s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10218, saving model to model:1_0.hdf5\n",
            "149/149 - 8s - loss: 0.4596 - accuracy: 0.0465 - val_loss: 0.1022 - val_accuracy: 0.0941\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10218 to 0.02734, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0506 - accuracy: 0.0764 - val_loss: 0.0273 - val_accuracy: 0.1004\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02734 to 0.02119, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0241 - accuracy: 0.0926 - val_loss: 0.0212 - val_accuracy: 0.1088\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02119 to 0.01993, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0204 - accuracy: 0.1009 - val_loss: 0.0199 - val_accuracy: 0.1174\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01993 to 0.01771, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0190 - accuracy: 0.1065 - val_loss: 0.0177 - val_accuracy: 0.1214\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01771 to 0.01731, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0180 - accuracy: 0.1168 - val_loss: 0.0173 - val_accuracy: 0.0980\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01731 to 0.01695, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0172 - accuracy: 0.1221 - val_loss: 0.0169 - val_accuracy: 0.1033\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01695 to 0.01653, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0168 - accuracy: 0.1280 - val_loss: 0.0165 - val_accuracy: 0.1079\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01653 to 0.01646, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0164 - accuracy: 0.1334 - val_loss: 0.0165 - val_accuracy: 0.1092\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01646 to 0.01623, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0160 - accuracy: 0.1424 - val_loss: 0.0162 - val_accuracy: 0.1430\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01623 to 0.01619, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0157 - accuracy: 0.1451 - val_loss: 0.0162 - val_accuracy: 0.1151\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01619 to 0.01586, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0154 - accuracy: 0.1498 - val_loss: 0.0159 - val_accuracy: 0.1245\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01586\n",
            "149/149 - 6s - loss: 0.0151 - accuracy: 0.1541 - val_loss: 0.0159 - val_accuracy: 0.1440\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01586\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "149/149 - 6s - loss: 0.0149 - accuracy: 0.1592 - val_loss: 0.0160 - val_accuracy: 0.1556\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01586 to 0.01556, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0142 - accuracy: 0.1717 - val_loss: 0.0156 - val_accuracy: 0.1287\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01556\n",
            "149/149 - 6s - loss: 0.0140 - accuracy: 0.1724 - val_loss: 0.0156 - val_accuracy: 0.1465\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01556 to 0.01551, saving model to model:1_0.hdf5\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
            "149/149 - 7s - loss: 0.0137 - accuracy: 0.1812 - val_loss: 0.0155 - val_accuracy: 0.1327\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01551 to 0.01542, saving model to model:1_0.hdf5\n",
            "149/149 - 7s - loss: 0.0134 - accuracy: 0.1859 - val_loss: 0.0154 - val_accuracy: 0.1478\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01542\n",
            "149/149 - 6s - loss: 0.0132 - accuracy: 0.1910 - val_loss: 0.0155 - val_accuracy: 0.1421\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01542\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000210199505e-05.\n",
            "149/149 - 6s - loss: 0.0131 - accuracy: 0.1915 - val_loss: 0.0154 - val_accuracy: 0.1314\n",
            "train:\n",
            "[('loss', 0.01155497133731842), ('accuracy', 0.25578710436820984)]\n",
            "val:\n",
            "[('loss', 0.015417186543345451), ('accuracy', 0.14780600368976593)]\n",
            "Training fold:  0\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0318s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10529, saving model to model:2_0.hdf5\n",
            "149/149 - 8s - loss: 0.4619 - accuracy: 0.0445 - val_loss: 0.1053 - val_accuracy: 0.0609\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10529 to 0.02848, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0518 - accuracy: 0.0684 - val_loss: 0.0285 - val_accuracy: 0.1113\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02848 to 0.01970, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0246 - accuracy: 0.0875 - val_loss: 0.0197 - val_accuracy: 0.1022\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01970 to 0.01856, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0204 - accuracy: 0.0961 - val_loss: 0.0186 - val_accuracy: 0.0892\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01856 to 0.01760, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0188 - accuracy: 0.1050 - val_loss: 0.0176 - val_accuracy: 0.1302\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01760 to 0.01717, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0179 - accuracy: 0.1138 - val_loss: 0.0172 - val_accuracy: 0.1371\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01717 to 0.01670, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0174 - accuracy: 0.1209 - val_loss: 0.0167 - val_accuracy: 0.1121\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01670 to 0.01648, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0170 - accuracy: 0.1244 - val_loss: 0.0165 - val_accuracy: 0.1430\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01648 to 0.01640, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0165 - accuracy: 0.1302 - val_loss: 0.0164 - val_accuracy: 0.1106\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01640 to 0.01614, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0161 - accuracy: 0.1375 - val_loss: 0.0161 - val_accuracy: 0.1514\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01614 to 0.01588, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0158 - accuracy: 0.1397 - val_loss: 0.0159 - val_accuracy: 0.1226\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01588 to 0.01577, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0155 - accuracy: 0.1463 - val_loss: 0.0158 - val_accuracy: 0.1562\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01577\n",
            "149/149 - 6s - loss: 0.0152 - accuracy: 0.1545 - val_loss: 0.0158 - val_accuracy: 0.1606\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01577\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "149/149 - 6s - loss: 0.0148 - accuracy: 0.1604 - val_loss: 0.0159 - val_accuracy: 0.1600\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01577 to 0.01550, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0142 - accuracy: 0.1726 - val_loss: 0.0155 - val_accuracy: 0.1619\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01550 to 0.01549, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0140 - accuracy: 0.1752 - val_loss: 0.0155 - val_accuracy: 0.1312\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01549 to 0.01546, saving model to model:2_0.hdf5\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
            "149/149 - 7s - loss: 0.0138 - accuracy: 0.1795 - val_loss: 0.0155 - val_accuracy: 0.1549\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01546 to 0.01539, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0135 - accuracy: 0.1856 - val_loss: 0.0154 - val_accuracy: 0.1337\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01539 to 0.01538, saving model to model:2_0.hdf5\n",
            "149/149 - 7s - loss: 0.0133 - accuracy: 0.1879 - val_loss: 0.0154 - val_accuracy: 0.1377\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01538 to 0.01535, saving model to model:2_0.hdf5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000210199505e-05.\n",
            "149/149 - 7s - loss: 0.0133 - accuracy: 0.1896 - val_loss: 0.0153 - val_accuracy: 0.1371\n",
            "train:\n",
            "[('loss', 0.011349000036716461), ('accuracy', 0.2502231001853943)]\n",
            "val:\n",
            "[('loss', 0.015345310792326927), ('accuracy', 0.13709846138954163)]\n",
            "Training fold:  0\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.0317s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10450, saving model to model:3_0.hdf5\n",
            "149/149 - 8s - loss: 0.4592 - accuracy: 0.0428 - val_loss: 0.1045 - val_accuracy: 0.0397\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10450 to 0.02483, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0504 - accuracy: 0.0728 - val_loss: 0.0248 - val_accuracy: 0.0695\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02483 to 0.02123, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0244 - accuracy: 0.0924 - val_loss: 0.0212 - val_accuracy: 0.0871\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02123 to 0.01868, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0203 - accuracy: 0.1056 - val_loss: 0.0187 - val_accuracy: 0.1092\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01868 to 0.01753, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0187 - accuracy: 0.1143 - val_loss: 0.0175 - val_accuracy: 0.1157\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01753 to 0.01729, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0179 - accuracy: 0.1207 - val_loss: 0.0173 - val_accuracy: 0.1172\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01729 to 0.01669, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0172 - accuracy: 0.1269 - val_loss: 0.0167 - val_accuracy: 0.0991\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01669\n",
            "149/149 - 6s - loss: 0.0168 - accuracy: 0.1298 - val_loss: 0.0167 - val_accuracy: 0.1268\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01669 to 0.01643, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0163 - accuracy: 0.1363 - val_loss: 0.0164 - val_accuracy: 0.1295\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01643 to 0.01620, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0160 - accuracy: 0.1428 - val_loss: 0.0162 - val_accuracy: 0.1300\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01620 to 0.01593, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0157 - accuracy: 0.1488 - val_loss: 0.0159 - val_accuracy: 0.1109\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01593 to 0.01582, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0155 - accuracy: 0.1493 - val_loss: 0.0158 - val_accuracy: 0.1165\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01582 to 0.01576, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0151 - accuracy: 0.1560 - val_loss: 0.0158 - val_accuracy: 0.1178\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01576\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "149/149 - 6s - loss: 0.0149 - accuracy: 0.1615 - val_loss: 0.0158 - val_accuracy: 0.1310\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01576 to 0.01563, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0142 - accuracy: 0.1746 - val_loss: 0.0156 - val_accuracy: 0.1249\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01563 to 0.01553, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0139 - accuracy: 0.1772 - val_loss: 0.0155 - val_accuracy: 0.1444\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01553\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
            "149/149 - 6s - loss: 0.0138 - accuracy: 0.1867 - val_loss: 0.0156 - val_accuracy: 0.1266\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01553 to 0.01545, saving model to model:3_0.hdf5\n",
            "149/149 - 7s - loss: 0.0134 - accuracy: 0.1901 - val_loss: 0.0154 - val_accuracy: 0.1279\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01545\n",
            "149/149 - 6s - loss: 0.0133 - accuracy: 0.1915 - val_loss: 0.0155 - val_accuracy: 0.1331\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01545\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000210199505e-05.\n",
            "149/149 - 6s - loss: 0.0131 - accuracy: 0.1958 - val_loss: 0.0155 - val_accuracy: 0.1419\n",
            "train:\n",
            "[('loss', 0.011568890884518623), ('accuracy', 0.24880583584308624)]\n",
            "val:\n",
            "[('loss', 0.015447606332600117), ('accuracy', 0.12786059081554413)]\n",
            "Training fold:  0\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.0318s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.09172, saving model to model:4_0.hdf5\n",
            "149/149 - 8s - loss: 0.4600 - accuracy: 0.0363 - val_loss: 0.0917 - val_accuracy: 0.0605\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.09172 to 0.02867, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0504 - accuracy: 0.0704 - val_loss: 0.0287 - val_accuracy: 0.0733\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02867 to 0.02120, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0244 - accuracy: 0.0872 - val_loss: 0.0212 - val_accuracy: 0.0813\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02120 to 0.01917, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0204 - accuracy: 0.1005 - val_loss: 0.0192 - val_accuracy: 0.0861\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01917 to 0.01774, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0187 - accuracy: 0.1086 - val_loss: 0.0177 - val_accuracy: 0.0987\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01774 to 0.01710, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0179 - accuracy: 0.1141 - val_loss: 0.0171 - val_accuracy: 0.1270\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01710 to 0.01681, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0174 - accuracy: 0.1183 - val_loss: 0.0168 - val_accuracy: 0.1041\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01681 to 0.01652, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0169 - accuracy: 0.1239 - val_loss: 0.0165 - val_accuracy: 0.1329\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01652 to 0.01637, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0164 - accuracy: 0.1305 - val_loss: 0.0164 - val_accuracy: 0.1146\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01637 to 0.01602, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0162 - accuracy: 0.1361 - val_loss: 0.0160 - val_accuracy: 0.1157\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01602\n",
            "149/149 - 6s - loss: 0.0158 - accuracy: 0.1403 - val_loss: 0.0160 - val_accuracy: 0.1201\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01602 to 0.01592, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0154 - accuracy: 0.1475 - val_loss: 0.0159 - val_accuracy: 0.1207\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01592 to 0.01584, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0151 - accuracy: 0.1514 - val_loss: 0.0158 - val_accuracy: 0.1358\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01584 to 0.01576, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0148 - accuracy: 0.1559 - val_loss: 0.0158 - val_accuracy: 0.1207\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01576 to 0.01565, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0146 - accuracy: 0.1629 - val_loss: 0.0156 - val_accuracy: 0.1421\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01565\n",
            "149/149 - 6s - loss: 0.0141 - accuracy: 0.1686 - val_loss: 0.0157 - val_accuracy: 0.1474\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01565 to 0.01554, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0139 - accuracy: 0.1736 - val_loss: 0.0155 - val_accuracy: 0.1304\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01554\n",
            "149/149 - 6s - loss: 0.0137 - accuracy: 0.1813 - val_loss: 0.0155 - val_accuracy: 0.1281\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01554\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "149/149 - 6s - loss: 0.0134 - accuracy: 0.1880 - val_loss: 0.0156 - val_accuracy: 0.1461\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01554 to 0.01539, saving model to model:4_0.hdf5\n",
            "149/149 - 7s - loss: 0.0127 - accuracy: 0.2030 - val_loss: 0.0154 - val_accuracy: 0.1415\n",
            "train:\n",
            "[('loss', 0.010193144902586937), ('accuracy', 0.3021363615989685)]\n",
            "val:\n",
            "[('loss', 0.015386822633445263), ('accuracy', 0.14150744676589966)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qpQSh1SDEwIH",
        "outputId": "196e6a6a-dd85-43bb-f73f-0137d5cdecd2"
      },
      "source": [
        "history.history['loss']\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe00ed57ba8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaeklEQVR4nO3df5Ac5X3n8fe3Z3ZmtTuLfuysQYBA4FPOETG2dWuCHQf7YkwEuCBxHAeSXMjZdcRJSJnzuRJSJMTGVbnD1OHCd0pinLgSu2yD7bPPKkcYY+IkkDOYRfwyxhhBwEhW8EoCVtJK2pnp7/3RPbuzo9ndkbQ7s/P051U1NT3dz8581Zr99LPP9NNj7o6IiPS+qNsFiIjI4lCgi4gEQoEuIhIIBbqISCAU6CIigch364XL5bKvX7++Wy8vItKTHnrooT3uPtJqW9cCff369YyNjXXr5UVEepKZPT/XNg25iIgEQoEuIhIIBbqISCAU6CIigVCgi4gEQoEuIhIIBbqISCB6LtAffG4fN33jB+iyvyIis/VcoD/6wsv85T8+w8ShardLERFZVnou0EeGigCMHzjS5UpERJaXngv0cikJ9L0KdBGRWXou0IdLBQD2HJjqciUiIstLzwV6vYe+Rz10EZFZei7QVw8UiExDLiIizXou0HORsWawwLiGXEREZum5QIdk2EVDLiIis/VsoGvIRURktp4M9OFSQWe5iIg06clA15CLiMjRejbQJ6dqTE5p+r+ISF1PBnp9ctFeDbuIiEzryUAfKel6LiIizXoy0Geu56IeuohIXU8G+sz1XNRDFxGp6+1A369AFxGp68lAL+ZznNSfZ+9BDbmIiNT1ZKBDMo6uD0VFRGb0dKBryEVEZEbvBvpQQUMuIiINejbQhwc1/V9EpFHPBnq5VOTlyQqVWtztUkREloXeDfSh5NTFfRp2EREB2gx0M9tsZk+Z2Q4zu26edr9iZm5mo4tXYmvDg+n0f30wKiICtBHoZpYDtgAXAxuBK81sY4t2Q8AHgAcWu8hWRtIeuj4YFRFJtNNDPw/Y4e7PuvsUcDtweYt2HwVuAg4vYn1zql/PRacuiogk2gn004AXGh7vTNdNM7NNwDp3//v5nsjMrjazMTMbGx8fP+ZiGw3XA11nuoiIAIvwoaiZRcAtwH9bqK273+buo+4+OjIyckKvO1jI0d8XachFRCTVTqDvAtY1PD49XVc3BPwM8I9m9hxwPrB1qT8YNbPkXHQNuYiIAO0F+oPABjM7y8wKwBXA1vpGd3/F3cvuvt7d1wP3A5e5+9iSVNygPKTruYiI1C0Y6O5eBa4B7gKeBL7o7k+Y2Y1mdtlSFzifkVJBX3IhIpLKt9PI3bcB25rW3TBH27edeFntGR4s8tjOVzr1ciIiy1rPzhSFmQt0xbF3uxQRka7r7UAvFanFziuHKt0uRUSk63o60HUuuojIjJ4O9HL63aI600VEpMcDfSTtoetMFxGRHg90DbmIiMzo6UBftaKPXGQKdBERejzQo8gYHtTkIhER6PFAh2TYRT10EZEAAr1cKjCuHrqISO8H+kipyF710EVEej/Qh0sF9hw4grum/4tItvV8oJdLRQ5XYg5O1bpdiohIVwUR6ICGXUQk83o+0IfT6f8600VEsq7nA73eQx/frzNdRCTbej7QR4bSIZeD6qGLSLb1fKCvGUyHXNRDF5GM6/lA78tFrBro0xi6iGRezwc6JOPoGnIRkawLItCHBwsachGRzAsi0MtDukCXiEgQgT6iKy6KiIQR6MODBSYOVzlS1fR/EcmuIAK9nJ6Lvu+gxtFFJLvCCPT6d4vqg1ERybAgAl3XcxERCSTQR+o9dAW6iGRYEIE+00PXkIuIZFcQgT5QyDNQyKmHLiKZFkSgQzr9X4EuIhkWUKAXNOQiIpkWTKAPa7aoiGRcMIFeLhXVQxeRTAso0AvsO3iEWuzdLkVEpCvaCnQz22xmT5nZDjO7rsX295vZ42b2iJndZ2YbF7/U+ZVLRWKHlybVSxeRbFow0M0sB2wBLgY2Ale2COzPu/tr3f31wMeAWxa90gXUp//v1bCLiGRUOz3084Ad7v6su08BtwOXNzZw94mGh4NAx8c9NP1fRLIu30ab04AXGh7vBH62uZGZ/T7wQaAA/EKrJzKzq4GrAc4444xjrXVeZU3/F5GMW7QPRd19i7u/Gvgj4E/maHObu4+6++jIyMhivTTQeD0XDbmISDa1E+i7gHUNj09P183lduCXTqSo43HSijx9OVMPXUQyq51AfxDYYGZnmVkBuALY2tjAzDY0PLwUeHrxSmyPmTE8WGTPfgW6iGTTgmPo7l41s2uAu4Ac8Gl3f8LMbgTG3H0rcI2ZXQhUgJeAq5ay6LmUhwrs1bcWiUhGtfOhKO6+DdjWtO6GhuUPLHJdx2V4UNP/RSS7gpkpCun0fw25iEhGhRXoQwX2HJzCXdP/RSR7wgr0wSJT1Zj9R6rdLkVEpOPCCvShdLaohl1EJIPCCvT69Vx0pouIZFBQgT48mM4WVQ9dRDIoqECfHnLRqYsikkFBBfqagQJmup6LiGRTUIGez0WsHiiohy4imRRUoEPyVXQKdBHJogADvahvLRKRTAou0IdLup6LiGRTcIFeLhXUQxeRTAow0IvsP1LlcKXW7VJERDoqwEDXuegikk0BBno6/V/DLiKSMcEGunroIpI1wQX6sIZcRCSjggv0mR66hlxEJFuCC/T+vhxDxbx66CKSOcEFOiTDLuqhi0jWBBnoyfR/9dBFJFuCDXQNuYhI1gQZ6BpyEZEsCjLQy6UiL01OUa3F3S5FRKRjwgz0oSLusG9SvXQRyY4wA30wnVy0X4EuItkRZqAPpddzOagPRkUkO8IMdF3PRUQyKMhAn76ei4ZcRCRDggz0oWKeQj5ij4ZcRCRDggx0M6M8WFAPXUQyJchAh+SDUY2hi0iWhBvopaLOchGRTAk20Ic15CIiGdNWoJvZZjN7ysx2mNl1LbZ/0My+b2aPmdk9Znbm4pd6bMpDSQ/d3btdiohIRywY6GaWA7YAFwMbgSvNbGNTs4eBUXc/F/gy8LHFLvRYlUtFKjVn4lC126WIiHREOz3084Ad7v6su08BtwOXNzZw92+7+2T68H7g9MUt89iV03PRx/XBqIhkRDuBfhrwQsPjnem6ubwPuLPVBjO72szGzGxsfHy8/SqPg2aLikjWLOqHomb2m8AocHOr7e5+m7uPuvvoyMjIYr70UeqBvlfXRReRjMi30WYXsK7h8enpulnM7ELgeuCt7t71bvH09H/10EUkI9rpoT8IbDCzs8ysAFwBbG1sYGZvAD4JXObuP1n8Mo/d6oECkSnQRSQ7Fgx0d68C1wB3AU8CX3T3J8zsRjO7LG12M1ACvmRmj5jZ1jmermNykbFmsKivohORzGhnyAV33wZsa1p3Q8PyhYtc16IolwrqoYtIZgQ7UxTS6f8KdBHJiMADvaAhFxHJjKADfbikKy6KSHYEHejlUpHJqRqTU5r+LyLhCzzQk3PRNblIRLIg8EBPZovqei4ikgWZCHT10EUkC8IO9CFN/xeR7Ag60NcMpoG+X4EuIuELOtCL+Rwn9efZe1BDLiISvqADHZKvotOHoiKSBeEH+mBRQy4ikgnhB/pQQUMuIpIJ4Qe6pv+LSEYEH+jDg0VenqxQqcXdLkVEZEkFH+j1c9H3adhFRAIXfqDXp//rg1ERCVwGAl2zRUUkGzIQ6Lqei4hkQ2YCXT10EQld8IE+UMjR3xcp0EUkeMEHupmlXxatIRcRCVvwgQ7JsIuu5yIioctIoBfYox66iAQuI4FeZK966CISuEwE+nApuUBXHHu3SxERWTKZCPRyqUgtdl4+VOl2KSIiSyYzgQ5o2EVEgpaJQB9Op//rTBcRCVkmAn1keraoznQRkXBlItA15CIiWZCJQF+5oo9cZJr+LyJBy0SgR5ExPFjQ9H8RCVomAh303aIiEr7MBPpwqcC4eugiErC2At3MNpvZU2a2w8yua7H9AjPbbmZVM3v34pd54kY0/V9EArdgoJtZDtgCXAxsBK40s41NzX4E/Dbw+cUucLGUh5IhF3dN/xeRMLXTQz8P2OHuz7r7FHA7cHljA3d/zt0fA+IlqHFRDA8WOFyJOThV63YpIiJLop1APw14oeHxznTdMTOzq81szMzGxsfHj+cpjpvORReR0HX0Q1F3v83dR919dGRkpJMvTXlI3y0qImFrJ9B3AesaHp+eruue4xgHHx5Mr+eyX2e6iEiY2gn0B4ENZnaWmRWAK4CtS1vWPB7/Mvz1hVA9tp72SNpD33tQPXQRCdOCge7uVeAa4C7gSeCL7v6Emd1oZpcBmNkbzWwn8KvAJ83siSWruH8V7BqDBz55TD+2Ju2h71EPXUQClW+nkbtvA7Y1rbuhYflBkqGYpbfhQthwEfzzzfC6K6HU3lh8Xy5i1UCfxtBFJFi9OVP0F/8cKpPwDx89ph8rl4oachGRYPVmoJc3wHm/A9s/A7sfa//HSgUNuYhIsHoz0AHe+ocwsAa+8cdtn/UyrAt0iUjAejfQV6yC/3g9PH8ffP9rbf3IiAJdRALWu4EOsOkqeNU5cPefQuXwgs3LpQITh6scqWr6v4iEp7cDPZeHzf8dXv4RfOd/L9h8eHr6v8bRRSQ8vR3oAGe/FV7zTrj3FpjYPW/TsgJdRALW+4EOcNFHIa7APTfO26xcSicXaRxdRAIURqCvORvO/1149POw66E5m9V76OMKdBEJUBiBDvDzH4LBV8Gd1815GqOGXEQkZOEEev9J8PYbYOd3kwt4tbCikGOwkNOQi4gEKZxAB3j9b8Da18G3/gymDrZsoslFIhKqsAI9imDz/4CJXfAvn2jZpFwqaMhFRIIUVqADnPlmOOeX4V9uhVd2HrW5rB66iAQqvEAHeMeNgMPdf3bUJg25iEiowgz0VWfAm/8Avvdl+NH9szaNlArsOzil6f8iEpwwAx3g566FobVw5x9BHE+vft26VcQO7/qL/8cz4we6WKCIyOIKN9CLJbjwI7D7EXj0C9Or3/7TJ/Op3xrlxy8f4p2fuI/bv/sj/Di+dFpEZLkJN9ABXvurcNoo3PMROLJ/evU7Np7MN669gE1nruK6rzzO731uOy9P6swXEeltYQd6FMHFN8GBF5OLdzU4+aR+Pvven+WPL34N33ryRS6+9V7uf3ZvlwoVETlxYQc6wOmjcO6vwXe2wEvPzdoURcbvvPXVfOV3f47+vhxXfup+br7rB1RqcevnEhFZxsIPdIALPwxRDr75py03v/b0lXz9D97Ce/7DOrZ8+xne/Vff4fm9rWeaiogsV9kI9JNOhbf8V3hyK/zrvS2bDBbz3PTuc9ny65v41/EDXHLrvXxl+059YCoiPSMbgQ7Jeekr1yVfKh3PfQ76peeu5c5rL+Cc01bywS8+yrV3PMLE4UoHCxUROT7ZCfS+FfCOj8CLj8P2z8zb9LRVK/jCfzmfD130U3z9sd1ccuu9PPT8vg4VKiJyfLIT6ADnvAvOeBN880/gjv8E930cnv0nOPzKUU1zkXHNL2zgS+9/E2bwnk/ez63fepqqPjAVkWXKujVGPDo66mNjY51/4X3Pwj0fTb7Z6OXnZ9YPb4BT3wCnbYJTN8Epr4XCAAD7D1e44WtP8NWHd/HG9av5+K+9ntNXD3S+dhHJPDN7yN1HW27LXKA3mtwHP94Oux5O7n/8MOxPv2jacvCqn54V8l/bvZLrt/6QQ5Uap5zUzykrk9vadHntyhWsXdXP2pX9jJSK5HPZ+gNIRJaeAv1YTOxOQz4N+B9vh0MvJdtyRY6UN/IUZzJeHWBPpcCLhwvsPJznpWo/B1jBhA9wgBUcZIAVpdUMrxpibRr8p65cwSkr+3nVUJHBYj65FXIMFPMM9OWIIuvuv11Elr35Aj3f6WKWvZPWwkmXwmsuTR67JxOS0h58cdfDnDt+XzLuHqdnv0RAocVzVaAy3seBPQPsj/uZ8BXs9wEm6OdFihzyIococJgih7xANddPnO+nlh/A8yugbwVRYQArDJArDJLrH6CvOEiuOEC+r0CU7yOX6yPf10dfZPTlIvI5o5CLyOci+nLJutnrjXwUkY+MXGTkc8l9zpL1uZyRj4zI0nsdZER6hgJ9IWaw5qzk9jO/Mntb9QgcnoAjE8m1Yur3h2ce9x2ZYPWR/aw6sp/q5MtUJieID09glX1E1UNEtUPkaofJx+m1ZGJgKr21KXajSkSVfHqfo0aOCjlqnt6To0qOChGHyVEjIsaIiXCM2G3244Zlt4Z7i8AiIF3GIKovp9ssSvabRbNuFkXJUJYZZoZZhBnJtul19fUGlhxYWm0nqj9vLrmPcrNfb/pxLn3dtG2Um35dMzCSg1aEY2ZE6frILNlmnrZNaqkf3yIzoiipK4qMKP03R1FElD7/dJv0ABmldeTS9snzRulrzvz7wGbee8nC7OXmbS0f00b7puVZ9yywjoYvY/fZy83bph83jQbU3z/198r0cnM9Uevl49ZUx1GjFAttb6i/uS5Lh1mP+jdFs9tF+eQ9usgU6CciX4TSSHJbgAF96a2luAaVQ+ltsuF+8qh1XjlErVohTm+1WgWvVYirVeLaFFarEtUqFGpVvFahP07uiatQq+BxDTyeucXJvXuM1dcRJ2/k9LE1bDNq4CSPiTH3me2ky/XDgvvMocPrhwqfviX7xomaf4lEQnbpLfDG9y360yrQl4sol1zyt1hasKkR8H+c+0xvrsV9HMfU4hqxx3gtxuOYOK7icY04ruFx3LBcgzieXo7rj2tV3IxanLxeDMRxcu8OsXtyi5O+Wpyuq2+ruSev4xDHcVJLWpt7TByn7eN0/fS2mTYOePqc08/lDsTEPrON+nM7OElBcexp2/TnPU7vPak3fS0cYpKDcpy8YLp9Zn86jseezohO93HD/4GnyzGkB3efrid2qKWvmyxbuu9ouDm16X2aHMaB6QN6NH1wJz3YzxzgzWYO+lFDJ2AxDv7e1MM/uo/evH324+b6kxqT+iNiIoOcQV8Euahh2SBvzobJM7jghP8VRws2F6RHTf/J3Vo6sCM9aDr44/RA0nSgrB9n6gfU5GBaP2g1rE8fOz598IsdmH5Ojnp+b17XUE9ygGw4IE4flNKDV9xQH04thmocU6k51VpMpZYup+sqtZhqen+o/rhp20+dum5J9rECXUQ6wszIWTJpT5ZGW50dM9tsZk+Z2Q4zu67F9qKZ3ZFuf8DM1i92oSIiMr8FA93McsAW4GJgI3ClmW1savY+4CV3/3fAx4GbFrtQERGZXzs99POAHe7+rLtPAbcDlze1uRz4u3T5y8DbzeYZCBURkUXXTqCfBrzQ8Hhnuq5lG3evAq8Aw4tRoIiItKejJwyY2dVmNmZmY+Pj4518aRGR4LUT6LuAxnNsTk/XtWxjZnlgJXDUNy67+23uPuruoyMjC0/GERGR9rUT6A8CG8zsLDMrAFcAW5vabAWuSpffDfyD67vbREQ6asHz0N29ambXAHcBOeDT7v6Emd0IjLn7VuBvgM+a2Q5gH0noi4hIB3Xt8rlmNg48v2DD1srAnkUsZ7GpvhOj+k7ccq9R9R2/M9295Zh11wL9RJjZ2FzXA14OVN+JUX0nbrnXqPqWhi6LISISCAW6iEggejXQb+t2AQtQfSdG9Z245V6j6lsCPTmGLiIiR+vVHrqIiDRRoIuIBGJZB/pyvg67ma0zs2+b2ffN7Akz+0CLNm8zs1fM7JH0dkOn6ktf/zkzezx97bEW283MPpHuv8fMbFMHa/v3DfvlETObMLNrm9p0fP+Z2afN7Cdm9r2GdWvM7G4zezq9Xz3Hz16VtnnazK5q1WYJarvZzH6Q/v991cxWzfGz874XlrjGD5vZrob/x0vm+Nl5f9+XsL47Gmp7zswemeNnO7IPT4inX7O03G4ks1KfAc4GCsCjwMamNr8H/FW6fAVwRwfrWwtsSpeHgB+2qO9twNe7uA+fA8rzbL8EuJPka0rPBx7o4v/1v5FMmOjq/gMuADYB32tY9zHgunT5OuCmFj+3Bng2vV+dLq/uQG0XAfl0+aZWtbXzXljiGj8MfKiN98C8v+9LVV/T9v8J3NDNfXgit+XcQ1/W12F3993uvj1d3g88ydGXFV7uLgc+44n7gVVmtrYLdbwdeMbdj3fm8KJx938muXxFo8b32d8Bv9TiR38RuNvd97n7S8DdwOalrs3dv+nJJasB7ie5eF7XzLH/2tHO7/sJm6++NDveA3xhsV+3U5ZzoPfMddjToZ43AA+02PwmM3vUzO40s3M6WljyZebfNLOHzOzqFtvb2cedcAVz/xJ1c//Vnezuu9PlfwNObtFmOezL95L8xdXKQu+FpXZNOiz06TmGrJbD/vt54EV3f3qO7d3ehwtazoHeE8ysBPwf4Fp3n2javJ1kGOF1wP8C/m+Hy3uLu28i+frA3zezCzr8+gtKr+B5GfClFpu7vf+O4snf3svuXF8zux6oAp+bo0k33wt/CbwaeD2wm2RYYzm6kvl758v+92k5B/qiXYd9qZhZH0mYf87dv9K83d0n3P1AurwN6DOzcqfqc/dd6f1PgK+S/FnbqJ19vNQuBra7+4vNG7q9/xq8WB+KSu9/0qJN1/almf028E7gN9IDzlHaeC8sGXd/0d1r7h4Dn5rjtbv6Xkzz413AHXO16eY+bNdyDvRlfR32dLztb4An3f2WOdqcUh/TN7PzSPZ3Rw44ZjZoZkP1ZZIPz77X1Gwr8Fvp2S7nA680DC10ypy9om7uvyaN77OrgK+1aHMXcJGZrU6HFC5K1y0pM9sM/CFwmbtPztGmnffCUtbY+LnML8/x2u38vi+lC4EfuPvOVhu7vQ/b1u1PZee7kZyF8UOST7+vT9fdSPLmBegn+VN9B/Bd4OwO1vYWkj+9HwMeSW+XAO8H3p+2uQZ4guQT+/uBN3ewvrPT1300raG+/xrrM2BLun8fB0Y7/P87SBLQKxvWdXX/kRxcdgMVknHc95F8LnMP8DTwLWBN2nYU+OuGn31v+l7cAfznDtW2g2Tsuf4erJ/1dSqwbb73Qgf332fT99djJCG9trnG9PFRv++dqC9d/7f1911D267swxO5aeq/iEgglvOQi4iIHAMFuohIIBToIiKBUKCLiARCgS4iEggFuohIIBToIiKB+P9xhtZYFv+2iwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "STrZGKE_Fsht",
        "outputId": "9a171d7a-3325-40e9-d95c-ff2c0f9fe0c1"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdd8a44ec18>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W43c7U9vV8Uf"
      },
      "source": [
        "def new_log_loss(y_tr, y_pr):\r\n",
        "  y_tr = y_tr.astype(np.float16)\r\n",
        "  y_pr = y_pr.astype(np.float16)\r\n",
        "  N, M = y_pr.shape\r\n",
        "  loss = []\r\n",
        "\r\n",
        "  for m in range(M):\r\n",
        "    loss_temp = 0\r\n",
        "    for i in range(N):\r\n",
        "      loss_temp -= ((y_tr[i,m]*np.log(y_pr[i,m]))+((1.0-y_tr[i,m])*np.log(1.0-y_pr[i,m])))\r\n",
        "    loss_temp = loss_temp/N\r\n",
        "    loss.append(loss_temp)\r\n",
        "    cleaned_loss = [l for l in loss if str(l) != 'nan']\r\n",
        "  return np.mean(cleaned_loss)\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93kD0BskZU0N",
        "outputId": "0ad50d80-920a-4d67-ec77-93456f0b9ee3"
      },
      "source": [
        "log_loss = 0.0\r\n",
        "count = 0\r\n",
        "y_tr_temp = y_train.to_numpy()\r\n",
        "for model in models:\r\n",
        "  y_pred = model.predict(x_train)\r\n",
        "  log_loss = log_loss + new_log_loss(y_tr_temp, y_pred)\r\n",
        "  count = count + 1\r\n",
        "print(\"Log loss: \", str(log_loss/count))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Log loss:  0.020168779337023397\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}